# -*- coding: utf-8 -*-
"""algorithms_da24b001.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13VPZG8vnCjpCK0Dn5VeaLQUk2oFPeaar
"""

#importing dependecies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time, os

TRAIN_CSV = '/content/MNIST_train.csv'
VAL_CSV = '/content/MNIST_validation.csv'
train_df = pd.read_csv(TRAIN_CSV)
val_df = pd.read_csv(VAL_CSV)
print('Train shape:', train_df.shape, 'Val shape:', val_df.shape)

train_df.head()

"""# EDA and DataPreprocessing"""

print(train_df['label'].value_counts().sort_index())

train_df.isnull().sum()

# Feature engineering
X_train_raw = train_df.drop('label', axis=1).values.astype(float) / 255.0
X_val_raw = val_df.drop('label', axis=1).values.astype(float) / 255.0
y_train = train_df['label'].values.astype(int)
y_val = val_df['label'].values.astype(int)

#tuning for vaiance threshold so that i can have save time
feature_variances = np.var(X_train_raw, axis=0)
print('\nVariance threshold tuning: features kept at various thresholds')
for thr in [1e-6, 1e-5, 1e-4, 1e-3]:
    kept = np.sum(feature_variances > thr)
    print(f'  threshold {thr:.0e}: {kept} features kept')
variance_threshold = 1e-5
mask = feature_variances > variance_threshold
X_train = X_train_raw[:, mask]
X_val = X_val_raw[:, mask]
print('\nFeatures after variance filtering:', X_train.shape[1])

#PCA
X_mean = np.mean(X_train, axis=0)
X_train_centered = X_train - X_mean
X_val_centered = X_val - X_mean

clip_limit = 3 * np.std(X_train_centered)
X_train_centered = np.clip(X_train_centered, -clip_limit, clip_limit)
X_val_centered = np.clip(X_val_centered, -clip_limit, clip_limit)

#using SVD
U, S, VT = np.linalg.svd(X_train_centered, full_matrices=False)
explained_variance = (S**2) / (X_train_centered.shape[0] - 1)
explained_variance_ratio = explained_variance / explained_variance.sum()
cumulative = np.cumsum(explained_variance_ratio)

def pick_n_components(threshold=0.95):
    return int(np.searchsorted(cumulative, threshold) + 1)

for th in [0.90, 0.92, 0.95, 0.98, 0.99]:
    nc = pick_n_components(th)
    print(f'  retain {th*100:.0f}% -> {nc} components')
n_components = pick_n_components(0.92)
print('\nSelected n_components =', n_components)

components = VT[:n_components].T
X_train_pca = X_train_centered.dot(components)
X_val_pca = X_val_centered.dot(components)
os.makedirs('/content/data', exist_ok=True)
np.save('/content/data/X_train_pca.npy', X_train_pca)
np.save('/content/data/X_val_pca.npy', X_val_pca)
np.save('/content/data/y_train.npy', y_train)
np.save('/content/data/y_val.npy', y_val)
np.save('/content/data/pca_components.npy', components)
np.save('/content/data/pca_mean.npy', X_mean)
np.save('/content/data/pca_mask.npy', mask)

print('Saved PCA artifacts to /content/data. PCA transformed shapes:', X_train_pca.shape, X_val_pca.shape)


import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (6,4)
plt.plot(np.arange(1, len(cumulative)+1), cumulative)
plt.axhline(0.92, color='red', linestyle='--')
plt.axvline(n_components, color='green', linestyle=':', label=f'n={n_components}')
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')
plt.grid(True)
plt.legend()
plt.show()

from collections import Counter, defaultdict
def one_hot(y, n_classes=None):
    y = np.array(y, dtype=int)
    if n_classes is None:
        n_classes = np.max(y) + 1
    oh = np.zeros((y.shape[0], n_classes), dtype=float)
    oh[np.arange(y.shape[0]), y] = 1.0
    return oh

def softmax(z):
    z = z - np.max(z, axis=1, keepdims=True)
    exp_z = np.exp(z)
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

def precision_recall_f1(y_true, y_pred, average='macro'):
    y_true = np.array(y_true, dtype=int)
    y_pred = np.array(y_pred, dtype=int)
    classes = np.unique(np.concatenate([y_true, y_pred]))
    precisions, recalls, f1s, supports = [], [], [], []
    for c in classes:
        tp = np.sum((y_pred == c) & (y_true == c))
        fp = np.sum((y_pred == c) & (y_true != c))
        fn = np.sum((y_pred != c) & (y_true == c))
        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0
        support = np.sum(y_true == c)
        precisions.append(prec); recalls.append(rec); f1s.append(f1); supports.append(support)
    if average == 'macro':
        return np.mean(precisions), np.mean(recalls), np.mean(f1s)
    elif average == 'weighted':
        weights = np.array(supports) / np.sum(supports)
        return np.sum(np.array(precisions) * weights), np.sum(np.array(recalls) * weights), np.sum(np.array(f1s) * weights)
    else:
        raise ValueError("average must be 'macro' or 'weighted'")

def f1_score(y_true, y_pred, average='macro'):
    return precision_recall_f1(y_true, y_pred, average=average)[2]

def confusion_matrix_plot(y_true, y_pred, title='Confusion Matrix'):
    y_true = np.array(y_true, dtype=int)
    y_pred = np.array(y_pred, dtype=int)
    classes = np.unique(np.concatenate([y_true, y_pred]))
    K = len(classes)
    cm = np.zeros((K, K), dtype=int)
    mapping = {c:i for i,c in enumerate(classes)}
    for t, p in zip(y_true, y_pred):
        cm[mapping[t], mapping[p]] += 1
    plt.imshow(cm, interpolation='nearest', cmap='Blues')
    plt.title(title)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.colorbar()
    ticks = np.arange(K)
    plt.xticks(ticks, classes)
    plt.yticks(ticks, classes)
    for i in range(K):
        for j in range(K):
            plt.text(j, i, str(cm[i,j]), horizontalalignment='center', color='black', fontsize=8)
    plt.show()

def evaluate_model(model, X_train, y_train, X_val, y_val, model_name='Model'):
    t0 = time.time()
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_val)
    eval_time = time.time() - t0
    acc = lambda a,b: np.mean(a==b)
    acc_train, acc_val = acc(y_train, y_train_pred), acc(y_val, y_val_pred)
    prec_m, rec_m, f1_m = precision_recall_f1(y_val, y_val_pred, 'macro')
    prec_w, rec_w, f1_w = precision_recall_f1(y_val, y_val_pred, 'weighted')
    print(f"\n=== {model_name} ===")
    print(f"Train Acc: {acc_train:.4f} | Val Acc: {acc_val:.4f}")
    print(f"Macro P={prec_m:.4f} R={rec_m:.4f} F1={f1_m:.4f}")
    print(f"Weighted P={prec_w:.4f} R={rec_w:.4f} F1={f1_w:.4f}")
    print(f"Eval time: {eval_time:.2f}s")
    # confusion matrix
    confusion_matrix_plot(y_val, y_val_pred, title=f'Confusion Matrix: {model_name}')
    return dict(Model=model_name, Train_Acc=acc_train, Val_Acc=acc_val, Precision_Macro=prec_m, Recall_Macro=rec_m, F1_Macro=f1_m, Precision_Weighted=prec_w, Recall_Weighted=rec_w, F1_Weighted=f1_w, Eval_Time_s=eval_time)

"""# Base Models i have choosed"""

X_train = np.load('/content/data/X_train_pca.npy')
X_val = np.load('/content/data/X_val_pca.npy')
y_train = np.load('/content/data/y_train.npy')
y_val = np.load('/content/data/y_val.npy')
print('Shapes:', X_train.shape, X_val.shape)
results_phase2a = []

def train_with_time_limit(clf, X, y, time_limit=300, fit_kwargs=None):
    fit_kwargs = fit_kwargs or {}
    t0 = time.time()
    if fit_kwargs:
        clf.fit(X, y, **fit_kwargs)
    else:
        clf.fit(X, y)
    t_elapsed = time.time() - t0
    if t_elapsed > time_limit:
        print(f"WARNING: training exceeded {time_limit}s ({t_elapsed:.1f}s)")
    return t_elapsed

"""**Multiclass Logistic regrssion using Softmax**"""

class MultinomialLogisticRegression:
    def __init__(self, learning_rate=0.05, n_epochs=50, batch_size=128, reg_lambda=0.001, random_state=42, early_stopping_rounds=5):
        self.learning_rate = learning_rate
        self.n_epochs = n_epochs
        self.batch_size = batch_size
        self.reg_lambda = reg_lambda
        self.random_state = random_state
        self.early_stopping_rounds = early_stopping_rounds
        self.W = None
    def _init(self, n_features, n_classes):
        rng = np.random.RandomState(self.random_state)
        self.W = 0.01 * rng.randn(n_features, n_classes)
    def fit(self, X, y, X_val=None, y_val=None):
        X = np.asarray(X); y = np.asarray(y, dtype=int)
        n_samples, n_features = X.shape
        n_classes = np.max(y) + 1
        self._init(n_features, n_classes)
        yoh = one_hot(y, n_classes)
        best_val = -np.inf; rounds = 0; bestW = self.W.copy()
        for epoch in range(self.n_epochs):
            perm = np.random.permutation(n_samples)
            Xs = X[perm]; ys = yoh[perm]
            for i in range(0, n_samples, self.batch_size):
                xb = Xs[i:i+self.batch_size]; yb = ys[i:i+self.batch_size]
                if xb.size==0: continue
                probs = softmax(xb @ self.W)
                grad = xb.T @ (probs - yb) / xb.shape[0]
                grad += self.reg_lambda * self.W
                self.W -= self.learning_rate * grad
            if X_val is not None and y_val is not None and self.early_stopping_rounds and y_val.size > 0:
                val_pred = self.predict(X_val)
                val_f1 = f1_score(y_val, val_pred, 'macro')
                if val_f1 > best_val + 1e-6:
                    best_val = val_f1; bestW = self.W.copy(); rounds = 0
                else:
                    rounds += 1
                    if rounds >= self.early_stopping_rounds:
                        self.W = bestW;
                        # print(f'Early stopping at epoch {epoch+1}')
                        break
        return self
    def predict_proba(self, X):
        return softmax(np.asarray(X) @ self.W)
    def predict(self, X):
        return np.argmax(self.predict_proba(X), axis=1)

print('=====Logistic Regression grid search=====')
best_lr = None; best_lr_score = -np.inf
for lr in [0.1, 0.05]:
    for epochs in [100, 150]:
        for batch in [64, 128]:
            params = {'learning_rate':lr,'n_epochs':epochs,'batch_size':batch,'reg_lambda':0.001}
            print('Training LR', params)
            model = MultinomialLogisticRegression(**params)
            t = train_with_time_limit(model, X_train, y_train, fit_kwargs={'X_val':X_val,'y_val':y_val})
            res = evaluate_model(model, X_train, y_train, X_val, y_val, model_name=f"LR {params}")
            res['Train_Time_s'] = t
            results_phase2a.append(res)
            if res['F1_Weighted'] > best_lr_score and t <= 300:
                best_lr_score = res['F1_Weighted']; best_lr = (model, params, res)
print('Best LR weighted F1:', best_lr_score)

best_lr = None; best_lr_score = -np.inf
for lr in [0.1, 0.05]:
    for epochs in [150, 200]:
        for batch in [64]:
            for reg in [0.01, 0.001, 0.0001]:
                params = {'learning_rate':lr,'n_epochs':epochs,'batch_size':batch,'reg_lambda':reg}
                print('Training LR', params)
                model = MultinomialLogisticRegression(**params)
                t = train_with_time_limit(model, X_train, y_train, fit_kwargs={'X_val':X_val,'y_val':y_val})
                res = evaluate_model(model, X_train, y_train, X_val, y_val, model_name=f"LR {params}")
                res['Train_Time_s'] = t
                results_phase2a.append(res)
                if res['F1_Weighted'] > best_lr_score and t <= 300:
                    print(f"*** NEW BEST LR MODEL FOUND (F1: {res['F1_Weighted']:.4f}) ***")
                    best_lr_score = res['F1_Weighted']; best_lr = (model, params, res)
print('Best LR weighted F1:', best_lr_score)

"""**DecisionTree**"""

class DecisionTreeNode:
    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):
        self.feature = feature; self.threshold = threshold; self.left = left; self.right = right; self.value = value
    def is_leaf(self): return self.value is not None
class DecisionTreeClassifier:
    def __init__(self, max_depth=12, min_samples_split=5):
        self.max_depth = max_depth; self.min_samples_split = min_samples_split; self.root = None
    def _gini(self, y):
        if len(y)==0: return 0.0
        counts = np.bincount(y); ps = counts / counts.sum(); return 1 - np.sum(ps**2)
    def _best_split(self, X, y):
        n_samples, n_features = X.shape
        if n_samples < self.min_samples_split: return None, None, 0.0
        parent = self._gini(y)
        best_gain=0; best_feat=None; best_thr=None
        for f in range(n_features):
            vals = X[:,f]
            thr_candidates = np.unique(vals)
            if len(thr_candidates)>50:
                thr_candidates = np.percentile(vals, np.linspace(0,100,50))
            for t in thr_candidates:
                left = vals <= t; right = ~left
                if left.sum()==0 or right.sum()==0: continue
                gl = self._gini(y[left]); gr = self._gini(y[right])
                n=len(y); nl=left.sum(); nr=right.sum()
                child = (nl/n)*gl + (nr/n)*gr
                gain = parent - child
                if gain>best_gain: best_gain=gain; best_feat=f; best_thr=t
        return best_feat, best_thr, best_gain
    def _build(self, X, y, depth=0):
        if len(np.unique(y))==1 or depth>=self.max_depth or len(y)<self.min_samples_split:
            return DecisionTreeNode(value=Counter(y).most_common(1)[0][0])
        f, t, gain = self._best_split(X,y)
        if f is None: return DecisionTreeNode(value=Counter(y).most_common(1)[0][0])
        left = self._build(X[X[:,f]<=t], y[X[:,f]<=t], depth+1)
        right = self._build(X[X[:,f]>t], y[X[:,f]>t], depth+1)
        return DecisionTreeNode(feature=f, threshold=t, left=left, right=right)
    def fit(self, X, y):
        self.root = self._build(np.asarray(X), np.asarray(y,dtype=int), 0); return self
    def _pred_one(self, x, node):
        if node.is_leaf(): return node.value
        return self._pred_one(x, node.left) if x[node.feature]<=node.threshold else self._pred_one(x, node.right)
    def predict(self, X):
        X = np.asarray(X); return np.array([self._pred_one(x, self.root) for x in X], dtype=int)

print('======Decision Tree tuning=====')
best_dt = None; best_dt_score = -np.inf
for depth in [10, 14]:
    for min_s in [5, 10]:
        print('DT depth', depth, 'min_samples', min_s)
        model = DecisionTreeClassifier(max_depth=depth, min_samples_split=min_s)
        t = train_with_time_limit(model, X_train, y_train)
        res = evaluate_model(model, X_train, y_train, X_val, y_val, model_name=f"DT depth={depth} min_s={min_s}")
        res['Train_Time_s'] = t
        results_phase2a.append(res)
        if res['F1_Weighted'] > best_dt_score and t <= 300:
            best_dt_score = res['F1_Weighted']; best_dt = (model, {'max_depth':depth,'min_samples_split':min_s}, res)
print('Best DT weighted F1:', best_dt_score)

"""**RandomForest**"""

class RandomForestClassifier:
    def __init__(self, n_estimators=10, max_depth=10, max_features='sqrt', sample_ratio=0.8, random_state=1):
        self.n_estimators=n_estimators; self.max_depth=max_depth; self.max_features=max_features; self.sample_ratio=sample_ratio; self.random_state=random_state
        self.trees=[]; self.feature_idx=[]
    def _k(self, m):
        if self.max_features=='sqrt': return max(1, int(np.sqrt(m)))
        if self.max_features=='log2': return max(1, int(np.log2(m)))
        if isinstance(self.max_features, float): return max(1, int(self.max_features*m))
        return min(m, int(self.max_features) if self.max_features is not None else m)
    def fit(self, X, y):
        X=np.asarray(X); y=np.asarray(y,dtype=int)
        n,m=X.shape; rng=np.random.RandomState(self.random_state)
        self.trees=[]; self.feature_idx=[]
        for i in range(self.n_estimators):
            idx = rng.randint(0,n,int(n*self.sample_ratio))
            k=self._k(m); feats=list(rng.choice(m,k,replace=False))
            t=DecisionTreeClassifier(max_depth=self.max_depth); t.fit(X[idx][:,feats], y[idx])
            self.trees.append(t); self.feature_idx.append(feats)
        return self
    def predict(self, X):
        X=np.asarray(X)
        preds=[t.predict(X[:,feats]) for t,feats in zip(self.trees,self.feature_idx)]
        preds=np.vstack(preds); final=[]
        for col in preds.T: final.append(Counter(col).most_common(1)[0][0])
        return np.array(final,dtype=int)

"""**AdaBoost**"""

class DecisionStump:
    def __init__(self): self.feature=None; self.threshold=None; self.left=None; self.right=None
    def fit(self,X,y,weights):
        X=np.asarray(X); y=np.asarray(y,dtype=int); n,m=X.shape
        best_err=np.inf
        for f in range(m):
            vals=X[:,f]; ths=np.unique(vals)
            if len(ths)>50: ths=np.percentile(vals, np.linspace(0,100,50))
            for t in ths:
                left=vals<=t; right=~left
                if left.sum()==0 or right.sum()==0: continue
                left_counts=defaultdict(float); right_counts=defaultdict(float)
                for lbl,w,lm in zip(y,weights,left):
                    if lm: left_counts[lbl]+=w
                    else: right_counts[lbl]+=w
                if not left_counts or not right_counts: continue
                lv=max(left_counts.items(), key=lambda kv:kv[1])[0]
                rv=max(right_counts.items(), key=lambda kv:kv[1])[0]
                preds=np.where(left, lv, rv)
                err=np.sum(weights*(preds!=y))
                if err<best_err: best_err=err; self.feature=f; self.threshold=t; self.left=lv; self.right=rv
        if self.feature is None:
            counts=defaultdict(float)
            for lbl,w in zip(y,weights): counts[lbl]+=w
            maj=max(counts.items(), key=lambda kv:kv[1])[0]
            self.feature=0; self.threshold=1e9; self.left=self.right=maj
        return self
    def predict(self,X): X=np.asarray(X); mask=X[:,self.feature]<=self.threshold; return np.where(mask,self.left,self.right)

class AdaBoostClassifier:
    def __init__(self,n_estimators=50): self.n_estimators=n_estimators; self.learners=[]; self.alphas=[]; self.classes_=None
    def fit(self,X,y):
        X=np.asarray(X); y=np.asarray(y,dtype=int); n=X.shape[0]; self.classes_=np.unique(y); K=len(self.classes_)
        w=np.ones(n)/n
        for m in range(self.n_estimators):
            stump=DecisionStump().fit(X,y,w)
            pred=stump.predict(X)
            err=np.sum(w*(pred!=y)); err=np.clip(err,1e-12,1-1e-12)
            alpha=np.log((1-err)/err)+np.log(K-1)
            w=w*np.exp(alpha*(pred!=y)); w=w/w.sum()
            self.learners.append(stump); self.alphas.append(alpha)
            if err<1e-12: break
        return self
    def predict(self,X):
        X=np.asarray(X); n=X.shape[0]; scores=np.zeros((n,len(self.classes_)))
        class_to_idx={c:i for i,c in enumerate(self.classes_)}
        for stump,alpha in zip(self.learners,self.alphas):
            preds=stump.predict(X)
            for i,p in enumerate(preds): scores[i,class_to_idx[p]]+=alpha
        return np.array([self.classes_[np.argmax(row)] for row in scores],dtype=int)

"""Not using adaboost beacuse of its very high time for training

**XGBoost**
"""

class XGBDecisionTreeNode:
    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):
        self.feature_index = feature_index
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value

    def is_leaf_node(self):
        return self.value is not None
class XGBoostClassifier:
    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3,
                 min_samples_split=2, reg_lambda=1.0):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.reg_lambda = reg_lambda
        self.trees = []
        self.base_pred = None
        self.n_sub_features = None

    def _sigmoid(self, z):
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))

    def _compute_initial_prediction(self, y):
        p = np.mean(y)
        p = np.clip(p, 1e-15, 1 - 1e-15)
        return np.log(p / (1 - p))

    def _compute_gradients_hessians(self, y_true, y_pred_proba):
        gradients = y_pred_proba - y_true
        hessians = y_pred_proba * (1 - y_pred_proba)
        return gradients, hessians

    def _calculate_leaf_value(self, g, h):
        G = np.sum(g)
        H = np.sum(h)
        return -G / (H + self.reg_lambda)

    def _calculate_gain(self, g, h, left_idx, right_idx):
        g_left, h_left = g[left_idx], h[left_idx]
        g_right, h_right = g[right_idx], h[right_idx]

        G_left, H_left = np.sum(g_left), np.sum(h_left)
        G_right, H_right = np.sum(g_right), np.sum(h_right)
        G_parent, H_parent = np.sum(g), np.sum(h)

        gain_left = (G_left**2) / (H_left + self.reg_lambda) if (H_left + self.reg_lambda) != 0 else 0.0
        gain_right = (G_right**2) / (H_right + self.reg_lambda) if (H_right + self.reg_lambda) != 0 else 0.0
        gain_parent = (G_parent**2) / (H_parent + self.reg_lambda) if (H_parent + self.reg_lambda) != 0 else 0.0

        gain = 0.5 * (gain_left + gain_right - gain_parent)
        return gain

    def _find_best_split(self, X, g, h):
        best_gain = -np.inf
        best_feat, best_thresh = None, None
        n_samples, n_features = X.shape

        if self.n_sub_features is None:
             self.n_sub_features = int(np.sqrt(n_features)) + 1

        feature_indices = np.random.choice(n_features, self.n_sub_features, replace=False)

        for feat_idx in feature_indices:
            if X.shape[0] > 1 and np.all(X[:, feat_idx] == X[0, feat_idx]):
                continue
            thresholds = np.unique(X[:, feat_idx])
            if len(thresholds) > 10:
                thresholds = np.percentile(X[:, feat_idx], np.linspace(0, 100, 10))

            for thresh in thresholds:
                left_idx = X[:, feat_idx] <= thresh
                right_idx = X[:, feat_idx] > thresh

                if len(g[left_idx]) == 0 or len(g[right_idx]) == 0:
                    continue
                gain = self._calculate_gain(g, h, left_idx, right_idx)

                if gain > best_gain:
                    best_gain = gain
                    best_feat = feat_idx
                    best_thresh = thresh

        return best_feat, best_thresh, best_gain

    def _build_tree(self, X, g, h, depth):
        n_samples = len(g)
        if (depth >= self.max_depth or
            n_samples < self.min_samples_split):
            return XGBDecisionTreeNode(value=self._calculate_leaf_value(g, h))

        best_feat, best_thresh, best_gain = self._find_best_split(X, g, h)

        if best_gain <= 0 or best_feat is None:
            return XGBDecisionTreeNode(value=self._calculate_leaf_value(g, h))

        left_idx = X[:, best_feat] <= best_thresh
        right_idx = X[:, best_feat] > best_thresh

        if np.sum(left_idx) == 0 or np.sum(right_idx) == 0:
            return XGBDecisionTreeNode(value=self._calculate_leaf_value(g, h))

        left = self._build_tree(X[left_idx], g[left_idx], h[left_idx], depth + 1)
        right = self._build_tree(X[right_idx], g[right_idx], h[right_idx], depth + 1)

        return XGBDecisionTreeNode(feature_index=best_feat, threshold=best_thresh, left=left, right=right)

    def fit(self, X, y):
        X = np.asarray(X)
        y = np.asarray(y)

        n_samples, n_features = X.shape
        self.n_sub_features = int(np.sqrt(n_features)) + 1
        self.trees = []
        self.base_pred = self._compute_initial_prediction(y)
        current_predictions_raw = np.full(n_samples, self.base_pred)
        for i in range(self.n_estimators):
            y_pred_proba = self._sigmoid(current_predictions_raw)
            g, h = self._compute_gradients_hessians(y, y_pred_proba)
            tree = self._build_tree(X, g, h, depth=0)
            self.trees.append(tree)
            update = self.learning_rate * self._predict_tree(X, tree)
            current_predictions_raw = current_predictions_raw + update

    def _traverse_tree(self, inputs, node):
        if node.is_leaf_node():
            return node.value
        if node.feature_index is None:
             return node.value
        if inputs[node.feature_index] <= node.threshold:
            return self._traverse_tree(inputs, node.left)
        else:
            return self._traverse_tree(inputs, node.right)

    def _predict_tree(self, X, tree):
        return np.array([self._traverse_tree(inputs, tree) for inputs in X])

    def predict_proba(self, X):
        X = np.asarray(X)
        raw_predictions = np.full(X.shape[0], self.base_pred)

        for tree in self.trees:
            raw_predictions = raw_predictions + self.learning_rate * self._predict_tree(X, tree)

        return self._sigmoid(raw_predictions)

    def predict(self, X):
        probas = self.predict_proba(X)
        return (probas >= 0.5).astype(int)
class MulticlassXGBoost:
    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3,
                 min_samples_split=2, reg_lambda=1.0):

        print(f"Initializing MulticlassXGBoost (OvR) with max_depth={max_depth}")
        self.n_classes = 10
        self.models_per_class = []
        self.base_params = {
            'n_estimators': n_estimators,
            'learning_rate': learning_rate,
            'max_depth': max_depth,
            'min_samples_split': min_samples_split,
            'reg_lambda': reg_lambda
        }

    def fit(self, X, y):
        X = np.asarray(X)
        y = np.asarray(y)

        for k in range(self.n_classes):
            print(f"--- Training model for class {k} ---")
            y_binary = (y == k).astype(int)
            model_k = XGBoostClassifier(**self.base_params)
            model_k.fit(X, y_binary)
            self.models_per_class.append(model_k)
        print("--- Multiclass XGBoost training complete ---")
        return self
    def predict(self, X):
        X = np.asarray(X)
        n_samples = X.shape[0]
        all_probas = np.zeros((n_samples, self.n_classes))
        for k in range(self.n_classes):
            probas_k = self.models_per_class[k].predict_proba(X)
            all_probas[:, k] = probas_k
        return np.argmax(all_probas, axis=1)

print('=====MulticlassXGBoost tuning=======')
best_xgb = None; best_xgb_score = -np.inf
for n_est in [20]:
    for lr in [0.1]:
        for depth in [3, 5]:

            params = {
                'n_estimators': n_est,
                'learning_rate': lr,
                'max_depth': depth,
                'min_samples_split': 10,
                'reg_lambda': 1.0
            }
            print('MulticlassXGB', params)
            model = MulticlassXGBoost(**params)
            t = train_with_time_limit(model, X_train, y_train, time_limit=300)
            res = evaluate_model(model, X_train, y_train, X_val, y_val, model_name=f"MulticlassXGB {params}")
            res['Train_Time_s'] = t
            results_phase2a.append(res)

            if res['F1_Weighted'] > best_xgb_score and t <= 300:
                best_xgb_score = res['F1_Weighted']; best_xgb = (model, params, res)
print('Best XGB weighted F1:', best_xgb_score)

#MulticlassXGBoost tuning 
best_xgb = None; best_xgb_score = -np.inf
for n_est in [40, 60]:
    for lr in [0.1]:
        for depth in [5]:

            params = {
                'n_estimators': n_est,
                'learning_rate': lr,
                'max_depth': depth,
                'min_samples_split': 10,
                'reg_lambda': 1.0
            }
            print('MulticlassXGB', params)

            model = MulticlassXGBoost(**params)
            t = train_with_time_limit(model, X_train, y_train, time_limit=300)
            res = evaluate_model(model, X_train, y_train, X_val, y_val, model_name=f"MulticlassXGB {params}")
            res['Train_Time_s'] = t
            results_phase2a.append(res)

            if res['F1_Weighted'] > best_xgb_score and t <= 300:
                best_xgb_score = res['F1_Weighted']; best_xgb = (model, params, res)
print('Best XGB weighted F1:', best_xgb_score)

#MulticlassXGBoost tuning 
best_xgb = best_xgb
best_xgb_score = best_xgb_score if 'best_xgb_score' in globals() else 0.9128
for n_est in [80]:
    for lr in [0.1]:
        for depth in [5, 6]:
            params = {
                'n_estimators': n_est,
                'learning_rate': lr,
                'max_depth': depth,
                'min_samples_split': 10,
                'reg_lambda': 1.0
            }
            print('MulticlassXGB', params)

            model = MulticlassXGBoost(**params)
            t = train_with_time_limit(model, X_train, y_train, time_limit=400)
            if t > 300:
                continue
            res = evaluate_model(model, X_train, y_train, X_val, y_val, model_name=f"MulticlassXGB {params}")
            res['Train_Time_s'] = t
            results_phase2a.append(res)
            if res['F1_Weighted'] > best_xgb_score:
                print(f"*** NEW BEST XGB MODEL FOUND (F1: {res['F1_Weighted']:.4f}) ***")
                best_xgb_score = res['F1_Weighted']; best_xgb = (model, params, res)
print('Best XGB weighted F1:', best_xgb_score)

best_xgb = best_xgb if 'best_xgb' in globals() else None
best_xgb_score = best_xgb_score if 'best_xgb_score' in globals() else 0.0
params = {
    'n_estimators': 30,
    'learning_rate': 0.3,
    'max_depth': 3,
    'min_samples_split': 10,
    'reg_lambda': 1.0
}
print('MulticlassXGB', params)

model = MulticlassXGBoost(**params)
t = train_with_time_limit(model, X_train, y_train, time_limit=300)
if t > 300:
    print(f"SKIPPING: Model training took {t:.0f}s (over 300s limit).")
else:
    res = evaluate_model(model, X_train, y_train, X_val, y_val, model_name=f"MulticlassXGB {params}")
    res['Train_Time_s'] = t
    results_phase2a.append(res)
    if res['F1_Weighted'] > best_xgb_score:
        print(f"*** NEW BEST XGB MODEL FOUND (F1: {res['F1_Weighted']:.4f}) ***")
        best_xgb_score = res['F1_Weighted']; best_xgb = (model, params, res)
    else:
        print(f"Friend's params (F1: {res['F1_Weighted']:.4f}) did not beat current best (F1: {best_xgb_score:.4f}).")
print('Best XGB weighted F1:', best_xgb_score)

df_a = pd.DataFrame(results_phase2a)
print('===summary ====')
print(df_a.sort_values('F1_Weighted', ascending=False).to_markdown(index=False, floatfmt='.4f'))

"""# Ensembling"""

results_phase2b = []
print('=======Bagging: Logistic Regression ensemble=============')
try:
    n_bag = 8
    rng = np.random.RandomState(0)
    preds = []
    t0 = time.time()
    for i in range(n_bag):
        idx = rng.choice(len(X_train), int(0.8*len(X_train)), replace=True)
        lr_params = best_lr[1] if 'best_lr' in globals() and best_lr else {'learning_rate':0.05, 'n_epochs':50, 'batch_size':128, 'reg_lambda':0.001}
        lr_model = MultinomialLogisticRegression(**lr_params)
        lr_model.fit(X_train[idx], y_train[idx])
        preds.append(lr_model.predict(X_val))
    t = time.time() - t0
    preds = np.vstack(preds)
    final = []
    for col in preds.T:
        final.append(Counter(col).most_common(1)[0][0])
    final = np.array(final)
    prec_m, rec_m, f1_m = precision_recall_f1(y_val, final, 'macro')
    prec_w, rec_w, f1_w = precision_recall_f1(y_val, final, 'weighted')
    res = dict(Model='Bagged LR', Train_Acc=None, Val_Acc=None, Precision_Macro=prec_m, Recall_Macro=rec_m, F1_Macro=f1_m, Precision_Weighted=prec_w, Recall_Weighted=rec_w, F1_Weighted=f1_w, Eval_Time_s=0.0)
    res['Train_Time_s']=t
    results_phase2b.append(res)
    print('Bagged LR done')
except Exception as e:
    print('Bagging LR failed:', e)

print('\n--- RF variant seeds ---')
rf_params = best_rf[1] if 'best_rf' in globals() and best_rf else {'n_estimators':40,'max_depth':10,'max_features':'sqrt','sample_ratio':0.8}
for seed in [2,3]:
    t0 = time.time()
    rf_params['random_state'] = seed
    rf2 = RandomForestClassifier(**rf_params)
    rf2.fit(X_train, y_train)
    t = time.time() - t0
    res = evaluate_model(rf2, X_train, y_train, X_val, y_val, model_name=f'RF_seed_{seed}')
    res['Train_Time_s'] = t
    results_phase2b.append(res)

df_b = pd.DataFrame(results_phase2b)
print('=========Phase 2B summary=====')
print(df_b.sort_values('F1_Weighted', ascending=False).to_markdown(index=False, floatfmt='.4f'))

"""# Stacking"""

def numpy_kfold_indices(n, n_splits=5, shuffle=True, random_state=0):
    idx = np.arange(n)
    if shuffle:
        rng = np.random.RandomState(random_state)
        rng.shuffle(idx)
    folds = np.array_split(idx, n_splits)
    return folds

if 'best_lr' not in globals() or 'best_xgb' not in globals() or not best_lr or not best_xgb:
    raise RuntimeError("Phase 2A did not run or failed to find best LR and XGB models. Cannot proceed with stacking.")

print("Using best tuned models for stacking:")
print(f"  LR: {best_lr[1]}")
print(f"  XGB: {best_xgb[1]}")

base_list = [
    ('lr', MultinomialLogisticRegression, best_lr[1]),
    ('xgb', MulticlassXGBoost, best_xgb[1])
]
n_splits = 5
n = X_train.shape[0]
folds = numpy_kfold_indices(n, n_splits=n_splits, shuffle=True, random_state=1)

meta_train = None
for name, EstClass, kwargs in base_list:
    print(f'Generating OOF features for: {name}')
    oof = np.zeros((n,))
    for i in range(n_splits):
        print(f'..Fold {i+1}/{n_splits}')
        hold = folds[i]
        train_idx = np.concatenate([f for j,f in enumerate(folds) if j!=i])
        est = EstClass(**kwargs)
        est.fit(X_train[train_idx], y_train[train_idx])
        oof[hold] = est.predict(X_train[hold])
    col = oof.reshape(-1,1)
    meta_train = col if meta_train is None else np.hstack([meta_train, col])
fitted_bases = []
for name, EstClass, kwargs in base_list:
    print(f'..Fitting {name} on full data')
    est = EstClass(**kwargs)
    est.fit(X_train, y_train)
    fitted_bases.append((name, est))

print('Fitting meta-learner...')
meta_clf = MultinomialLogisticRegression(learning_rate=0.1, n_epochs=80, batch_size=128, reg_lambda=0.001)
meta_clf.fit(meta_train, y_train)

meta_val = None
for name, est in fitted_bases:
    col = est.predict(X_val).reshape(-1,1)
    meta_val = col if meta_val is None else np.hstack([meta_val, col])

meta_preds = meta_clf.predict(meta_val)
prec_m, rec_m, f1_m = precision_recall_f1(y_val, meta_preds, 'macro')
prec_w, rec_w, f1_w = precision_recall_f1(y_val, meta_preds, 'weighted')
print('\nStacked Ensemble results:')
print(f'  Macro F1: {f1_m:.4f} | Weighted F1: {f1_w:.4f}')
confusion_matrix_plot(y_val, meta_preds, title='Confusion Matrix: Stacked Ensemble')
# Store stacked result for final comparison
stacked_res = dict(Model='Stacked Ensemble', Train_Acc=None, Val_Acc=None, Precision_Macro=prec_m, Recall_Macro=rec_m, F1_Macro=f1_m, Precision_Weighted=prec_w, Recall_Weighted=rec_w, F1_Weighted=f1_w, Eval_Time_s=0.0, Train_Time_s=0.0)


print('\n--- Phase 2C: Stacking Ensemble (FIXED with One-Hot Encoding) ---')
print('--- Using only the TWO best models: LR and MulticlassXGBoost ---')

def numpy_kfold_indices(n, n_splits=5, shuffle=True, random_state=0):
    idx = np.arange(n)
    if shuffle:
        rng = np.random.RandomState(random_state)
        rng.shuffle(idx)
    folds = np.array_split(idx, n_splits)
    return folds
def one_hot_stack_features(label_array, n_classes=10):
    n_samples, n_models = label_array.shape
    ohe_features = np.zeros((n_samples, n_models * n_classes))
    for i in range(n_models):
        labels = label_array[:, i]
        ohe = one_hot(labels, n_classes=n_classes)
        ohe_features[:, i*n_classes : (i+1)*n_classes] = ohe
    return ohe_features

if 'best_lr' not in globals() or 'best_xgb' not in globals() or not best_lr or not best_xgb:
    raise RuntimeError("Phase 2A did not run or failed to find best LR and XGB models. Cannot proceed with stacking.")

print("Using best tuned models for stacking:")
print(f"  LR: {best_lr[1]}")
print(f"  XGB: {best_xgb[1]}")

base_list = [
    ('lr', MultinomialLogisticRegression, best_lr[1]),
    ('xgb', MulticlassXGBoost, best_xgb[1])
]
n_splits = 5
n = X_train.shape[0]
folds = numpy_kfold_indices(n, n_splits=n_splits, shuffle=True, random_state=1)

meta_train_labels = None
for name, EstClass, kwargs in base_list:
    print(f'Generating OOF features for: {name}')
    oof = np.zeros((n,))
    for i in range(n_splits):
        print(f'..Fold {i+1}/{n_splits}')
        hold = folds[i]
        train_idx = np.concatenate([f for j,f in enumerate(folds) if j!=i])
        est = EstClass(**kwargs)
        est.fit(X_train[train_idx], y_train[train_idx])
        oof[hold] = est.predict(X_train[hold])
    col = oof.reshape(-1,1)
    meta_train_labels = col if meta_train_labels is None else np.hstack([meta_train_labels, col])

print('Fitting base models on full train data for validation...')
fitted_bases = []
for name, EstClass, kwargs in base_list:
    print(f'..Fitting {name} on full data')
    est = EstClass(**kwargs)
    est.fit(X_train, y_train)
    fitted_bases.append((name, est))

print('Fitting meta-learner...')
meta_val_labels = None
for name, est in fitted_bases:
    col = est.predict(X_val).reshape(-1,1)
    meta_val_labels = col if meta_val_labels is None else np.hstack([meta_val_labels, col])
print('One-Hot Encoding meta-features...')
meta_train_ohe = one_hot_stack_features(meta_train_labels)
meta_val_ohe = one_hot_stack_features(meta_val_labels)
print(f'Meta-feature shape is now: {meta_train_ohe.shape}')


meta_clf = MultinomialLogisticRegression(learning_rate=0.1, n_epochs=80, batch_size=128, reg_lambda=0.001)
meta_clf.fit(meta_train_ohe, y_train)
meta_preds = meta_clf.predict(meta_val_ohe)
prec_m, rec_m, f1_m = precision_recall_f1(y_val, meta_preds, 'macro')
prec_w, rec_w, f1_w = precision_recall_f1(y_val, meta_preds, 'weighted')
print('\nStacked Ensemble results:')
print(f'  Macro F1: {f1_m:.4f} | Weighted F1: {f1_w:.4f}')
confusion_matrix_plot(y_val, meta_preds, title='Confusion Matrix: Stacked Ensemble')
stacked_res = dict(Model='Stacked Ensemble', Train_Acc=None, Val_Acc=None, Precision_Macro=prec_m, Recall_Macro=rec_m, F1_Macro=f1_m, Precision_Weighted=prec_w, Recall_Weighted=rec_w, F1_Weighted=f1_w, Eval_Time_s=0.0, Train_Time_s=0.0)



import pickle

candidates = []
if 'df_a' in globals():
    candidates.append(df_a)
if 'df_b' in globals():
    candidates.append(df_b)
if 'stacked_res' in globals():
    candidates.append(pd.DataFrame([stacked_res]))

if len(candidates)>0:
    combined = pd.concat(candidates, ignore_index=True, sort=False).fillna(-1)
    if 'F1_Weighted' in combined.columns:
        combined_sorted = combined.sort_values('F1_Weighted', ascending=False)
    else:
        combined_sorted = combined
    print('\nTop candidates (All Models):')
    print(combined_sorted.to_markdown(index=False, floatfmt='.4f'))
else:
    print('No candidate tables found; stacking will be used by default if available')

# Choose final model:
print('--- FINAL MODEL SELECTION ---')
print('Based on results, the best model is the Stacked Ensemble (F1 ~0.911) or the MulticlassXGBoost (F1 ~0.913).')
print('The Stacked Ensemble will be the final model, as it is the most robust.')
final_model_info = {'type':'stacked'}
print('\nFinal model choice:', final_model_info)


# Save stacked components
os.makedirs('/content/artifacts', exist_ok=True)
try:
    with open('/content/artifacts/stacked_meta_clf.pkl', 'wb') as f:
        pickle.dump(meta_clf, f)
    print('Saved stacked meta_clf (for reference) to /content/artifacts/stacked_meta_clf.pkl')
except Exception as e:
    print('Warning: could not save meta_clf pickle:', e)
if 'best_lr' not in globals() or 'best_xgb' not in globals() or not best_lr or not best_xgb:
    print("WARNING: Best models from Phase 2A not found (LR, XGB). Using defaults for main.py. Re-run Phase 2A.")
    best_lr_params = {'learning_rate':0.05,'n_epochs':30,'batch_size':128,'reg_lambda':0.001}
    best_xgb_params = {'n_estimators':60, 'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'reg_lambda': 1.0} # Fallback xgb params
else:
    best_lr_params = best_lr[1]
    best_xgb_params = best_xgb[1]
if 'min_samples_split' not in best_xgb_params:
    best_xgb_params['min_samples_split'] = 2
if 'reg_lambda' not in best_xgb_params:
    best_xgb_params['reg_lambda'] = 1.0
base_list_str = f"""
base_list = [
    ('lr', MultinomialLogisticRegression, {best_lr_params}),
    ('xgb', MulticlassXGBoost, {best_xgb_params})
]
"""
main_py = """# main.py - auto-generated final runner
import numpy as np
import pandas as pd
import time, os
from collections import Counter, defaultdict

def one_hot(y, n_classes=None):
    y = np.array(y, dtype=int)
    if n_classes is None:
        n_classes = np.max(y) + 1
    oh = np.zeros((y.shape[0], n_classes), dtype=float)
    oh[np.arange(y.shape[0]), y] = 1.0
    return oh

def softmax(z):
    z = z - np.max(z, axis=1, keepdims=True)
    exp_z = np.exp(z)
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

def one_hot_stack_features(label_array, n_classes=10):
    n_samples, n_models = label_array.shape
    ohe_features = np.zeros((n_samples, n_models * n_classes))
    for i in range(n_models):
        labels = label_array[:, i]
        ohe = one_hot(labels, n_classes=n_classes)
        ohe_features[:, i*n_classes : (i+1)*n_classes] = ohe
    return ohe_features

#Multinomial Logistic Regression
class MultinomialLogisticRegression:
    def __init__(self, learning_rate=0.05, n_epochs=50, batch_size=128, reg_lambda=0.001, random_state=42, early_stopping_rounds=5):
        self.learning_rate = learning_rate
        self.n_epochs = n_epochs
        self.batch_size = batch_size
        self.reg_lambda = reg_lambda
        self.random_state = random_state
        self.early_stopping_rounds = early_stopping_rounds
        self.W = None
    def _init(self, n_features, n_classes):
        rng = np.random.RandomState(self.random_state)
        self.W = 0.01 * rng.randn(n_features, n_classes)
    def fit(self, X, y, X_val=None, y_val=None):
        X = np.asarray(X); y = np.asarray(y, dtype=int)
        n_samples, n_features = X.shape
        n_classes = np.max(y) + 1
        if n_features == 0:
            print("Warning: 0 features passed to LR.fit. Check OHE.")
            n_features = X.shape[1]
        self._init(n_features, n_classes)
        yoh = one_hot(y, n_classes)
        for epoch in range(self.n_epochs):
            perm = np.random.permutation(n_samples)
            Xs = X[perm]; ys = yoh[perm]
            for i in range(0, n_samples, self.batch_size):
                xb = Xs[i:i+self.batch_size]; yb = ys[i:i+self.batch_size]
                if xb.size==0: continue
                probs = softmax(xb @ self.W)
                grad = xb.T @ (probs - yb) / xb.shape[0]
                grad += self.reg_lambda * self.W
                self.W -= self.learning_rate * grad
        return self
    def predict_proba(self, X):
        return softmax(np.asarray(X) @ self.W)
    def predict(self, X):
        return np.argmax(self.predict_proba(X), axis=1)

#PowerfulXGBoost
class XGBDecisionTreeNode:
    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):
        self.feature_index = feature_index
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value
    def is_leaf_node(self):
        return self.value is not None

class XGBoostClassifier:
    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3,
                 min_samples_split=2, reg_lambda=1.0):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.reg_lambda = reg_lambda
        self.trees = []
        self.base_pred = None
        self.n_sub_features = None
    def _sigmoid(self, z):
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))
    def _compute_initial_prediction(self, y):
        p = np.mean(y)
        p = np.clip(p, 1e-15, 1 - 1e-15)
        return np.log(p / (1 - p))
    def _compute_gradients_hessians(self, y_true, y_pred_proba):
        gradients = y_pred_proba - y_true
        hessians = y_pred_proba * (1 - y_pred_proba)
        return gradients, hessians
    def _calculate_leaf_value(self, g, h):
        G = np.sum(g)
        H = np.sum(h)
        return -G / (H + self.reg_lambda)
    def _calculate_gain(self, g, h, left_idx, right_idx):
        g_left, h_left = g[left_idx], h[left_idx]
        g_right, h_right = g[right_idx], h[right_idx]
        G_left, H_left = np.sum(g_left), np.sum(h_left)
        G_right, H_right = np.sum(g_right), np.sum(h_right)
        G_parent, H_parent = np.sum(g), np.sum(h)
        gain_left = (G_left**2) / (H_left + self.reg_lambda) if (H_left + self.reg_lambda) != 0 else 0.0
        gain_right = (G_right**2) / (H_right + self.reg_lambda) if (H_right + self.reg_lambda) != 0 else 0.0
        gain_parent = (G_parent**2) / (H_parent + self.reg_lambda) if (H_parent + self.reg_lambda) != 0 else 0.0
        gain = 0.5 * (gain_left + gain_right - gain_parent)
        return gain
    def _find_best_split(self, X, g, h):
        best_gain = -np.inf
        best_feat, best_thresh = None, None
        n_samples, n_features = X.shape
        if self.n_sub_features is None:
             self.n_sub_features = int(np.sqrt(n_features)) + 1
        feature_indices = np.random.choice(n_features, self.n_sub_features, replace=False)
        for feat_idx in feature_indices:
            if X.shape[0] > 1 and np.all(X[:, feat_idx] == X[0, feat_idx]):
                continue
            thresholds = np.unique(X[:, feat_idx])
            if len(thresholds) > 10:
                thresholds = np.percentile(thresholds, np.linspace(0, 100, 10))
            for thresh in thresholds:
                left_idx = X[:, feat_idx] <= thresh
                right_idx = X[:, feat_idx] > thresh
                if len(g[left_idx]) == 0 or len(g[right_idx]) == 0:
                    continue
                gain = self._calculate_gain(g, h, left_idx, right_idx)
                if gain > best_gain:
                    best_gain = gain
                    best_feat = feat_idx
                    best_thresh = thresh
        return best_feat, best_thresh, best_gain
    def _build_tree(self, X, g, h, depth):
        n_samples = len(g)
        if (depth >= self.max_depth or n_samples < self.min_samples_split):
            return XGBDecisionTreeNode(value=self._calculate_leaf_value(g, h))
        best_feat, best_thresh, best_gain = self._find_best_split(X, g, h)
        if best_gain <= 0 or best_feat is None:
            return XGBDecisionTreeNode(value=self._calculate_leaf_value(g, h))
        left_idx = X[:, best_feat] <= best_thresh
        right_idx = X[:, best_feat] > best_thresh
        if np.sum(left_idx) == 0 or np.sum(right_idx) == 0:
            return XGBDecisionTreeNode(value=self._calculate_leaf_value(g, h))
        left = self._build_tree(X[left_idx], g[left_idx], h[left_idx], depth + 1)
        right = self._build_tree(X[right_idx], g[right_idx], h[right_idx], depth + 1)
        return XGBDecisionTreeNode(feature_index=best_feat, threshold=best_thresh, left=left, right=right)
    def fit(self, X, y):
        X = np.asarray(X); y = np.asarray(y)
        n_samples, n_features = X.shape
        self.n_sub_features = int(np.sqrt(n_features)) + 1
        self.trees = []
        self.base_pred = self._compute_initial_prediction(y)
        current_predictions_raw = np.full(n_samples, self.base_pred)
        for i in range(self.n_estimators):
            y_pred_proba = self._sigmoid(current_predictions_raw)
            g, h = self._compute_gradients_hessians(y, y_pred_proba)
            tree = self._build_tree(X, g, h, depth=0)
            self.trees.append(tree)
            update = self.learning_rate * self._predict_tree(X, tree)
            current_predictions_raw = current_predictions_raw + update
        return self
    def _traverse_tree(self, inputs, node):
        if node.is_leaf_node():
            return node.value
        if node.feature_index is None:
             return node.value
        if inputs[node.feature_index] <= node.threshold:
            return self._traverse_tree(inputs, node.left)
        else:
            return self._traverse_tree(inputs, node.right)
    def _predict_tree(self, X, tree):
        return np.array([self._traverse_tree(inputs, tree) for inputs in X])
    def predict_proba(self, X):
        X = np.asarray(X)
        raw_predictions = np.full(X.shape[0], self.base_pred)
        for tree in self.trees:
            raw_predictions = raw_predictions + self.learning_rate * self._predict_tree(X, tree)
        return self._sigmoid(raw_predictions)
    def predict(self, X):
        probas = self.predict_proba(X)
        return (probas >= 0.5).astype(int)

# Multiclass XGBoost Wrapper
class MulticlassXGBoost:
    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3,
                 min_samples_split=2, reg_lambda=1.0):
        self.n_classes = 10
        self.models_per_class = []
        self.base_params = {{
            'n_estimators': n_estimators,
            'learning_rate': learning_rate,
            'max_depth': max_depth,
            'min_samples_split': min_samples_split,
            'reg_lambda': reg_lambda
        }}
    def fit(self, X, y):
        X = np.asarray(X); y = np.asarray(y)
        for k in range(self.n_classes):
            print("--- Training model for class " + str(k) + " ---")
            y_binary = (y == k).astype(int)
            model_k = XGBoostClassifier(**self.base_params)
            model_k.fit(X, y_binary)
            self.models_per_class.append(model_k)
        print("Multiclass XGBoost training completed")
        return self
    def predict(self, X):
        X = np.asarray(X)
        n_samples = X.shape[0]
        all_probas = np.zeros((n_samples, self.n_classes))
        for k in range(self.n_classes):
            probas_k = self.models_per_class[k].predict_proba(X)
            all_probas[:, k] = probas_k
        return np.argmax(all_probas, axis=1)
def load_and_preprocess(train_file_path, test_file_path):
        train_df = pd.read_csv(train_file_path)
    X_train_raw = train_df.drop('label', axis=1).values.astype(float) / 255.0
    y_train = train_df['label'].values.astype(int)

        test_df = pd.read_csv(test_file_path)

    if 'label' in test_df.columns:
        X_test_raw = test_df.drop('label', axis=1).values.astype(float)/255.0
    else:
        X_test_raw = test_df.values.astype(float)/255.0
    variance_threshold = 1e-5
    feature_variances = np.var(X_train_raw, axis=0)
    mask = feature_variances > variance_threshold

    X_train_filt = X_train_raw[:, mask]
    X_test_filt = X_test_raw[:, mask]
    X_mean = np.mean(X_train_filt, axis=0)
    X_train_centered = X_train_filt - X_mean
    X_test_centered = X_test_filt - X_mean

    clip_limit = 3 * np.std(X_train_centered)
    X_train_centered = np.clip(X_train_centered, -clip_limit, clip_limit)
    X_test_centered = np.clip(X_test_centered, -clip_limit, clip_limit)
    U, S, VT = np.linalg.svd(X_train_centered, full_matrices=False)
    explained_variance = (S**2) / (X_train_centered.shape[0] - 1)
    explained_variance_ratio = explained_variance / explained_variance.sum()
    cumulative = np.cumsum(explained_variance_ratio)

    n_components = int(np.searchsorted(cumulative, 0.92) + 1)
    print("PCA retaining 92% variance -> " + str(n_components) + " components.")
    components = VT[:n_components].T
    X_train_pca = X_train_centered.dot(components)
    X_test_pca = X_test_centered.dot(components)
    return X_train_pca, y_train, X_test_pca

def main():
    TRAIN_CSV = 'MNIST_train.csv'
    TEST_CSV = 'MNIST_test.csv'
    OUT_PRED = 'predictions.csv'
    start_total = time.time()

    X_train_pca, y_train, X_test_pca = load_and_preprocess(TRAIN_CSV, TEST_CSV)
    if X_train_pca is None:
        return
    {base_list_str}
    fitted = []
    for name, EstClass, kwargs in base_list:
        print("Training final base model: " + str(name) + " with " + str(kwargs))
        est = EstClass(**kwargs)
        est.fit(X_train_pca, y_train)
        fitted.append((name, est))
    meta_X_labels = [est.predict(X_train_pca).reshape(-1,1) for _, est in fitted]
    meta_X = np.hstack(meta_X_labels)

    print("Original meta-feature shape: " + str(meta_X.shape))
    meta_X_ohe = one_hot_stack_features(meta_X)
    print("New OHE meta-feature shape: " + str(meta_X_ohe.shape))

    meta_clf = MultinomialLogisticRegression(learning_rate=0.1, n_epochs=80, batch_size=128, reg_lambda=0.001)
    meta_clf.fit(meta_X_ohe, y_train) # Fit on OHE features

    meta_test_labels = [est.predict(X_test_pca).reshape(-1,1) for _, est in fitted]
    meta_test_X = np.hstack(meta_test_labels)
    meta_test_X_ohe = one_hot_stack_features(meta_test_X)

    preds = meta_clf.predict(meta_test_X_ohe)
    out_df = pd.DataFrame({{'Id': np.arange(len(preds)), 'Label': preds}})
    out_df.to_csv(OUT_PRED, index=False)

    end_total = time.time()
    print("\\nSaved predictions to " + str(OUT_PRED))
    print("Total script runtime: " + str(round(end_total - start_total, 2)) + " seconds.")

if __name__ == "__main__":
    main()
"""

try:
    with open('/content/main_generated.py', 'w') as f:
        f.write(main_py.format(base_list_str=base_list_str))

    print('Generated /content-/main_generated.py  copy to main.py for submission')
    print('This file is now self-contained and will re-run the full PCA pipeline.')
except Exception as e:
    print('Failed to generate main.py:', e)